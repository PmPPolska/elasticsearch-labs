{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llamaindex_processor.py\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "class LlamaIndexProcessor:\n",
    "   def __init__(self):\n",
    "       pass \n",
    "   \n",
    "   def load_documents(self, directory_path):\n",
    "       ''' \n",
    "       Load all documents in directory\n",
    "       '''\n",
    "       reader = SimpleDirectoryReader(input_dir=directory_path)\n",
    "       return reader.load_data()\n",
    "\n",
    "# main.ipynb\n",
    "# 1. Clean up all the .lab files \n",
    "# 2. Recurse through each resume directory, standardize the names. \n",
    "llamaindex_processor=LlamaIndexProcessor()\n",
    "documents=llamaindex_processor.load_documents('./dataset/resumes_corpus/')\n",
    "documents=[dict(doc_obj) for doc_obj in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id_': '78bc0417-c60c-4878-ae2d-831a5dd67f2f',\n",
       " 'embedding': None,\n",
       " 'metadata': {'file_path': '/Users/han/Desktop/searchlabs/resume/elasticsearch-labs/supporting-blog-content/resume/dataset/resumes_corpus/00001.txt',\n",
       "  'file_name': '00001.txt',\n",
       "  'file_type': 'text/plain',\n",
       "  'file_size': 8163,\n",
       "  'creation_date': '2024-09-10',\n",
       "  'last_modified_date': '2020-06-20'},\n",
       " 'excluded_embed_metadata_keys': ['file_name',\n",
       "  'file_type',\n",
       "  'file_size',\n",
       "  'creation_date',\n",
       "  'last_modified_date',\n",
       "  'last_accessed_date'],\n",
       " 'excluded_llm_metadata_keys': ['file_name',\n",
       "  'file_type',\n",
       "  'file_size',\n",
       "  'creation_date',\n",
       "  'last_modified_date',\n",
       "  'last_accessed_date'],\n",
       " 'relationships': {},\n",
       " 'text': 'Database Administrator <span class=\"hl\">Database</span> <span class=\"hl\">Administrator</span> Database Administrator - Family Private Care LLC Lawrenceville, GA A self-motivated Production SQL Server Database Administrator who possesses strong analytical and problem solving skills. My experience includes SQL Server 2005, 2008 and 2012, 2014, SSIS, as well as clustering, mirroring, and high availability solutions in OLTP environments. I am proficient in database backup, recovery, performance tuning, maintenance tasks, security, and consolidation. I am confident that I would make a beneficial addition to any company. Over the course of my career thus far, I have designed databases to fit a variety of needs, successfully ensured the security of those databases, problem-solved in order to meet both back-end and front-end needs, installed and tested new versions database management systems, customized and installed applications and meticulously monitored performance for the smoothest front-end experience possible. During my 5 to 6 years working with databases. Work Experience Database Administrator Family Private Care LLC - Roswell, GA April 2017 to Present Confirm that backups have been made and successfully saved to a secure location Planning for backup and recovery of database information. Maintaining archived data Backing up and restoring databases, Contacting database vendor for technical support Generating various reports by querying from database as per needed. Managing and monitoring data replication. Acting as liaison with users High Availability or Disaster Recovery Logs - Check your high availability and/or disaster recovery process logs. Depending on the solution (Log Shipping, Clustering, Replication, Database Mirroring, CDP, etc.) that you are using dictates what needs to be checked. Correcting errors and make necessary modification Modify existing databases and database management systems or direct programmers and analysts to make changes. Work as part of a project team to coordinate database development and see termite project scope and limitations Train Users and answers questions. Approve, schedule, plan, and supervise the installation and testing of new products and improvements to computer systems, such as the installation of new databases. Review Procedures in Database management system manuals for making changes to database. Select and enter codes to monitor database performance and to create production database. Check the backup failure alerts, correct the errors and rerun the backups. Review the average duration of backup, any significant changes occurred investigates on this. Most of the time it happens due to networking low bandwidth Validate the backup files using restore verify only. I create jobs to take care of the task and to send a notification if it fails to verify any backup. Monitoring all backup and log history is cleaning when designed. Find out the newly added databases and define the backup plan. I do verify the free space on each drive on all servers, If there is significant variance in free space from the day before, research the cause of the free space fluctuation and resolve if necessary, Often times, log files will grow because of monthly jobs automate through a job. The job runs for every one hour and reports any drive which is having less than 15 % of free space. I can design a SSRS report to showcase and review the delta values. Confirm all servers/databases are up and running fine. Usually in an Enterprise Database Environment Third Party Tools are used to monitor Servers. For database monitoring, I can design a native SQL Server solution using T-SQL code and a maintenance plan, it run min by min and send an email to DBA team if it is not able to connect to any of the database in the instance. I do design a native scripts using T-SQL to monitor Replication, Mirroring, Log shipping. Database Administrator Incomm Alpharetta - Alpharetta, GA January 2014 to February 2017 * Responsible for administering and maintaining over 150 database servers of Production and Test environment. * Analyzes the current database environment to determine recommended database maintenance, security, and Microsoft SQL Server best practices. * Monitors and troubleshoots production environments using Idera SQLdm. * Creates and maintains documentation for DBA standard operating procedures. * Ensures that all code changes made in the production environment are SOX compliance before they are deployed. * Analyses and migrates data using ETL into SQL Server databases to support customer\\'s implementation. * Works closely with infrastructure team for patching and hardware upgrades, and ensures that both production and test servers are up to date by applying Windows and SQL Server patches. * Upgrades servers as required from SQL Server 2005 to SQL Server 2008, 2012., and 2014. * Completes database administration maintenance projects as required * Provides 24/7 on call support as needed. * Coordinates and configures new nodes for production server clusters for high availability. * Periodically restores backup files in test environment to check for corruption. * Reviews security, performance, and disk space and recommend corrective actions where needed. * Assists application team in the creation of databases, construction of queries, modification of database tables, and troubleshooting data issues. * Monitors servers for resources utilization (disk space, memory, CPU, etc.) * Creates and Executes a migration/decommission plan for over 500 databases within a 4 month project window. Education Bachelor of Science Lead City University July 2013 Skills Database administration, Database, Ms sql server, Ms sql server 2005, Sql server, Sql server 2005, Sql server 2008, Sql server 2008 r2, Sql server 2012, Sql, Sql queries, Stored procedures, Clustering, Backups, T-sql, Virtualization, R2, Maintenance, Problem solving, Shipping Additional Information SKILLS Installation and Building Server Running Backups Recovering and Restoring Models Support various MS SQL Server MS SQL Server 2005/2008 environments from SQL Server /2008R2R2/2012/2014 2005 thru SQL Server 2008r2 as administration including well as with SQL Server 2012 on installation, configuration, Windows Server 2003, 2003r2, upgrades, capacity planning, 2008, 2008r2, 2012 and 2014. performance tuning, backup and recovery. Familiar with virtualization and Work with developers to Identify, managing SQL databases in a debug, and tune problem stored virtual environment. procedures, T-SQL Queries, etc. Management of users including Knowledge of High Availability of creation/alteration, grant of database servers using Database system/DB roles and permissions mirroring, replications, Log on various database objects. Shipping and Always On Availability Group. Design and implement SQL Server 201, databases for mission-critical 2005, SQL Server 2008 R2, business. Experience in creating tables, Environment transition planning views, indexes, stored procedures (development, test, stage, and more. production). Import and export data to and from Perform routine backup and the database servers. recovery testing and documenting recovery scenarios. Database administration including Use SQL Profiler for installation, configuration, troubleshooting, monitoring, and upgrades, capacity planning, optimization of SQL Server and performance tuning, backup and SQL code. recovery, index maintenance. Modify stored procedures, queries, Manage the production and views, indexes, and functions to development databases including handle business rules, history data performance tuning, capacity and audit analysis. planning, SQL Server clustering, database security configuration, and database continuity. Excellent analytical, Strong decision making and communication skills, work ethics problem solving skills. and ability to work in a team environment with strong determination and commitment to the deliverables. Highly motivated self-starter with the ability to work independently.',\n",
       " 'mimetype': 'text/plain',\n",
       " 'start_char_idx': None,\n",
       " 'end_char_idx': None,\n",
       " 'text_template': '{metadata_str}\\n\\n{content}',\n",
       " 'metadata_template': '{key}: {value}',\n",
       " 'metadata_seperator': '\\n'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from itertools import combinations\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "class NLTKProcessor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get_word_pos(self, word):\n",
    "        # Tokenize the word (this step is necessary even for a single word)\n",
    "        tokens = word_tokenize(word)\n",
    "        \n",
    "        # Perform part-of-speech tagging\n",
    "        tagged = pos_tag(tokens)\n",
    "        \n",
    "        # Return the tag (we're assuming there's only one word)\n",
    "        return tagged[0][1]\n",
    "    \n",
    "    def get_wordnet_pos(self, treebank_tag):\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return 'a'\n",
    "        else:\n",
    "            # As default pos in lemmatization is Noun\n",
    "            return wordnet.NOUN\n",
    "        \n",
    "    def split_into_sentences(self, text):\n",
    "        return sent_tokenize(text)\n",
    "        \n",
    "    def textrank_phrases(self, text, top_n=10, phrase_length=3, mode='phrase'):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        # Tokenize the text into sentences\n",
    "        sentences = sent_tokenize(text)\n",
    "\n",
    "        # Tokenize sentences into words and remove stopwords\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        words = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "        words = [[lemmatizer.lemmatize(word, pos=self.get_wordnet_pos(self.get_word_pos(word))) for word in sentence if word.isalnum() and word not in stop_words] for sentence in words]\n",
    "\n",
    "        # Part-of-speech tagging\n",
    "        tagged = [pos_tag(sentence) for sentence in words]\n",
    "\n",
    "        # Extract nouns and adjectives\n",
    "        if mode == 'phrase':\n",
    "            keywords = [[word for word, pos in sentence if pos.startswith('NN') or pos.startswith('JJ')] for sentence in tagged]\n",
    "        else:  # mode == 'sentence'\n",
    "            keywords = [' '.join(sentence) for sentence in words]\n",
    "\n",
    "        # Generate phrases\n",
    "        if mode == 'phrase':\n",
    "            phrases = []\n",
    "            for sentence in keywords:\n",
    "                phrases.extend([' '.join(phrase) for phrase in zip(*[sentence[i:] for i in range(phrase_length)])])\n",
    "        else:\n",
    "            phrases = keywords\n",
    "\n",
    "        # Build the graph\n",
    "        graph = nx.Graph()\n",
    "        if mode == 'phrase':\n",
    "            # For phrases, use co-occurrence within sentences\n",
    "            for sentence_phrases in [phrases[i:i+len(keywords[j])-phrase_length+1] for j, i in enumerate([sum(len(k)-phrase_length+1 for k in keywords[:j]) for j in range(len(keywords))])]:\n",
    "                for pair in combinations(sentence_phrases, 2):\n",
    "                    if graph.has_edge(*pair):\n",
    "                        graph[pair[0]][pair[1]]['weight'] += 1\n",
    "                    else:\n",
    "                        graph.add_edge(*pair, weight=1)\n",
    "        else:\n",
    "            # For sentences, use similarity between sentences\n",
    "            for pair in combinations(range(len(phrases)), 2):\n",
    "                similarity = len(set(phrases[pair[0]].split()) & set(phrases[pair[1]].split())) / \\\n",
    "                            (len(set(phrases[pair[0]].split()) | set(phrases[pair[1]].split())) + 1e-6)\n",
    "                if similarity > 0:\n",
    "                    graph.add_edge(pair[0], pair[1], weight=similarity)\n",
    "\n",
    "        # Apply PageRank\n",
    "        scores = nx.pagerank(graph)\n",
    "\n",
    "        # Sort by score and return top n\n",
    "        sorted_items = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        if mode == 'phrase':\n",
    "            return [item for item, score in sorted_items[:top_n]]\n",
    "        else:\n",
    "            return [sentences[idx] for idx, score in sorted_items[:top_n]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
