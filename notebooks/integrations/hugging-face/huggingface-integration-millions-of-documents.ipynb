{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a765629",
   "metadata": {},
   "source": [
    "# Semantic Search using the Inference API with the Hugging Face Inference Endpoints Service\n",
    "\n",
    "TODO [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/elastic/elasticsearch-labs/blob/main/notebooks/search/TODO)\n",
    "\n",
    "\n",
    "Learn how to use the [Inference API](https://www.elastic.co/guide/en/elasticsearch/reference/current/inference-apis.html) with the Hugging Face Inference Endpoint service for semantic search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9101eb9",
   "metadata": {},
   "source": [
    "# üß∞ Requirements\n",
    "\n",
    "For this example, you will need:\n",
    "\n",
    "- An Elastic deployment:\n",
    "   - We'll be using [Elastic Cloud](https://www.elastic.co/guide/en/cloud/current/ec-getting-started.html) for this example (available with a [free trial](https://cloud.elastic.co/registration?utm_source=github&utm_content=elasticsearch-labs-notebook))\n",
    "\n",
    "- Elasticsearch 8.14 or above.\n",
    "   \n",
    "- A paid [Hugging Face Inference Endpoint](https://huggingface.co/docs/inference-endpoints/guides/create_endpoint) is required to use the Inference API with \n",
    "the Hugging Face Inference Endpoint service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd69cc0",
   "metadata": {},
   "source": [
    "# Create Elastic Cloud deployment or serverless project\n",
    "\n",
    "If you don't have an Elastic Cloud deployment, sign up [here](https://cloud.elastic.co/registration?utm_source=github&utm_content=elasticsearch-labs-notebook) for a free trial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27dffbf",
   "metadata": {},
   "source": [
    "# Install packages and connect with Elasticsearch Client\n",
    "\n",
    "To get started, we'll need to connect to our Elastic deployment using the Python client (version 8.12.0 or above).\n",
    "Because we're using an Elastic Cloud deployment, we'll use the **Cloud ID** to identify our deployment.\n",
    "\n",
    "First we need to `pip` install the following packages:\n",
    "\n",
    "- `elasticsearch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c4b16bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: elasticsearch in /Users/mh/.venv/lib/python3.11/site-packages (8.14.0)\n",
      "Requirement already satisfied: elastic-transport<9,>=8.13 in /Users/mh/.venv/lib/python3.11/site-packages (from elasticsearch) (8.13.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.2 in /Users/mh/.venv/lib/python3.11/site-packages (from elastic-transport<9,>=8.13->elasticsearch) (2.1.0)\n",
      "Requirement already satisfied: certifi in /Users/mh/.venv/lib/python3.11/site-packages (from elastic-transport<9,>=8.13->elasticsearch) (2023.11.17)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: datasets in /Users/mh/.venv/lib/python3.11/site-packages (2.20.0)\n",
      "Requirement already satisfied: filelock in /Users/mh/.venv/lib/python3.11/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/mh/.venv/lib/python3.11/site-packages (from datasets) (1.23.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/mh/.venv/lib/python3.11/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Users/mh/.venv/lib/python3.11/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/mh/.venv/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/mh/.venv/lib/python3.11/site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/mh/.venv/lib/python3.11/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/mh/.venv/lib/python3.11/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /Users/mh/.venv/lib/python3.11/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /Users/mh/.venv/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /Users/mh/.venv/lib/python3.11/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.5.0)\n",
      "Requirement already satisfied: aiohttp in /Users/mh/.venv/lib/python3.11/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /Users/mh/.venv/lib/python3.11/site-packages (from datasets) (0.24.2)\n",
      "Requirement already satisfied: packaging in /Users/mh/.venv/lib/python3.11/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/mh/.venv/lib/python3.11/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/mh/.venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/mh/.venv/lib/python3.11/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/mh/.venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/mh/.venv/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/mh/.venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/mh/.venv/lib/python3.11/site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mh/.venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mh/.venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mh/.venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mh/.venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2023.11.17)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/mh/.venv/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/mh/.venv/lib/python3.11/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/mh/.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install elasticsearch\n",
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ef96b3",
   "metadata": {},
   "source": [
    "Next, we need to import the modules we need. üîê NOTE: getpass enables us to securely prompt the user for credentials without echoing them to the terminal, or storing it in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "690ff9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mh/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch, helpers, exceptions\n",
    "from urllib.request import urlopen\n",
    "from getpass import getpass\n",
    "import json\n",
    "import time\n",
    "from timeit import default_timer as timer\n",
    "import datasets\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fa2b6c",
   "metadata": {},
   "source": [
    "Now we can instantiate the Python Elasticsearch client.\n",
    "\n",
    "First we prompt the user for their password and Cloud ID.\n",
    "Then we create a `client` object that instantiates an instance of the `Elasticsearch` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "195cc597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.elastic.co/search-labs/tutorials/install-elasticsearch/elastic-cloud#finding-your-cloud-id\n",
    "ELASTIC_CLOUD_ID = getpass(\"Elastic Cloud ID: \")\n",
    "\n",
    "# https://www.elastic.co/search-labs/tutorials/install-elasticsearch/elastic-cloud#creating-an-api-key\n",
    "ELASTIC_API_KEY = getpass(\"Elastic Api Key: \")\n",
    "\n",
    "# Create the client instance\n",
    "client = Elasticsearch(\n",
    "    # For local development\n",
    "    # hosts=[\"http://localhost:9200\"]\n",
    "    cloud_id=ELASTIC_CLOUD_ID,\n",
    "    api_key=ELASTIC_API_KEY,\n",
    "    request_timeout=120, \n",
    "    max_retries=10,\n",
    "    retry_on_timeout=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1115ffb",
   "metadata": {},
   "source": [
    "### Test the Client\n",
    "Before you continue, confirm that the client has connected with this test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc0de5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'instance-0000000001', 'cluster_name': 'dd945ac31a6c456686ee5117f1ec38a2', 'cluster_uuid': 'puJnzmQtThKzbfH1e3_bVw', 'version': {'number': '8.14.3', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': 'd55f984299e0e88dee72ebd8255f7ff130859ad0', 'build_date': '2024-07-07T22:04:49.882652950Z', 'build_snapshot': False, 'lucene_version': '9.10.0', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'}\n"
     ]
    }
   ],
   "source": [
    "print(client.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659c5890",
   "metadata": {},
   "source": [
    "Refer to [the documentation](https://www.elastic.co/guide/en/elasticsearch/client/python-api/current/connecting.html#connect-self-managed-new) to learn how to connect to a self-managed deployment.\n",
    "\n",
    "Read [this page](https://www.elastic.co/guide/en/elasticsearch/client/python-api/current/connecting.html#connect-self-managed-new) to learn how to connect using API keys."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840d92f0",
   "metadata": {},
   "source": [
    "<a name=\"create-the-inference-endpoint\"></a>\n",
    "## Create the inference endpoint object\n",
    "\n",
    "Let's create the inference endpoint by using the [Create inference API](https://www.elastic.co/guide/en/elasticsearch/reference/current/put-inference-api.html).\n",
    "\n",
    "You'll need an Hugging Face API key (access token) for this that you can find in your Hugging Face account under the [Access Tokens](https://huggingface.co/settings/tokens).\n",
    "\n",
    "You will also need to have created a [Hugging Face Inference Endpoint service instance](https://huggingface.co/docs/inference-endpoints/guides/create_endpoint) and noted the `url` of your instance. For this notebook, we deployed the `multilingual-e5-small` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d007737",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = getpass(\"Huggingface API key:  \")\n",
    "try:\n",
    "    client.inference.delete_model(inference_id=\"my_hf_endpoint_object\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "client.inference.put_model(\n",
    "    inference_id='my_hf_endpoint_object',\n",
    "    body={\n",
    "        \"service\": \"hugging_face\",\n",
    "        \"service_settings\": {\"api_key\": API_KEY, \n",
    "                             \"url\": \"<URL-TO-HUGGING-FACE-ENDPOINT>\",\n",
    "                            \"similarity\": \"dot_product\"\n",
    "                            },\n",
    "    },\n",
    "    task_type=\"text_embedding\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f4201d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.inference.inference(\n",
    "     inference_id='my_hf_endpoint_object',\n",
    "     input=\"this is the raw text of my document!\"\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2e48b7",
   "metadata": {},
   "source": [
    "**IMPORTANT:** If you use Elasticsearch 8.12, you must change `inference_id` in the snippet above to `model_id`! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0346151d",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1024d070",
   "metadata": {},
   "source": [
    "## Create an ingest pipeline with an inference processor\n",
    "\n",
    "Create an ingest pipeline with an inference processor by using the [`put_pipeline`](https://www.elastic.co/guide/en/elasticsearch/reference/master/put-pipeline-api.html) method. Reference the `inference_id` created above as `model_id` to infer on the data that is being ingested by the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ace9e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.ingest.put_pipeline(\n",
    "    id=\"hf_pipeline\",\n",
    "    processors=[\n",
    "        {\n",
    "            \"inference\": {\n",
    "                \"model_id\": \"my_hf_endpoint_object\",\n",
    "                \"input_output\": {\n",
    "                    \"input_field\": \"text\",\n",
    "                    \"output_field\": \"text_embedding\",\n",
    "                },\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d07567",
   "metadata": {},
   "source": [
    "Let's note a few important parameters from that API call:\n",
    "\n",
    "- `inference`: A processor that performs inference using a machine learning model.\n",
    "- `model_id`: Specifies the ID of the inference endpoint to be used. In this example, the inference ID is set to `my_hf_endpoint_object`. Use the inference ID you defined when created the inference task.\n",
    "- `input_output`: Specifies input and output fields.\n",
    "- `input_field`: Field name from which the `dense_vector` representation is created.\n",
    "- `output_field`:  Field name which contains inference results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e12d7a",
   "metadata": {},
   "source": [
    "## Create index\n",
    "\n",
    "The mapping of the destination index - the index that contains the embeddings that the model will create based on your input text - must be created. The destination index must have a field with the [dense_vector](https://www.elastic.co/guide/en/elasticsearch/reference/current/dense-vector.html) field type to index the output of the model we deployed in Hugging Face (`multilingual-e5-small`).\n",
    "\n",
    "Let's create an index named `hf-endpoint-index` with the mappings we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddcbca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.indices.create(\n",
    "    index=\"hf-endpoint-index\",\n",
    "    settings = {\n",
    "        \"index\": {\n",
    "            \"number_of_replicas\": \"1\",\n",
    "            \"number_of_shards\": \"1\",\n",
    "            \"default_pipeline\": \"hf_pipeline\",\n",
    "        }\n",
    "    },\n",
    "    mappings = {\n",
    "    \"properties\": {\n",
    "        \"text\": {\"type\": \"text\"},\n",
    "        \"text_embedding\": {\n",
    "            \"type\": \"dense_vector\",\n",
    "            \"dims\": 384,\n",
    "            \"similarity\": \"dot_product\",\n",
    "        },\n",
    "    },\n",
    "    \"_source\": {\"excludes\": [\"passage_embedding.predicted_value\"]},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12aa3452",
   "metadata": {},
   "source": [
    "## If you are using Elasticsearch serverless or v8.15+ then you will have access to the new `semantic_text` field\n",
    "`semantic_text` has significantly faster ingest times and is recommended.\n",
    "\n",
    "https://github.com/elastic/elasticsearch/blob/main/docs/reference/mapping/types/semantic-text.asciidoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eeb3755",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.indices.create(\n",
    "    index=\"hf-semantic-text-index\",\n",
    "    mappings={\n",
    "        \"properties\": {\n",
    "            \"infer_field\": {\n",
    "                \"type\": \"semantic_text\",\n",
    "                \"inference_id\": \"my_hf_endpoint_object\"\n",
    "            }\n",
    "        }\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c187a9",
   "metadata": {},
   "source": [
    "## Insert Documents\n",
    "\n",
    "In this example, we want to show the power of using GPUs in Hugging Face's Inference Endpoint service by indexing millions of multilingual documents from the miracl corpus. The speed at which these documents ingest will depend on whether you use a semantic text field (faster) or an ingest pipeline (slower) and will also depend on how much hardware your rent for your Hugging Face inference endpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68737cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = ['ar', 'bn', 'en', 'es', 'fa', 'fi', 'fr', 'hi', 'id', 'ja', 'ko', 'ru', 'sw', 'te', 'th', 'zh']\n",
    "\n",
    "\n",
    "all_langs_datasets = [iter(datasets.load_dataset('miracl/miracl-corpus', lang)['train']) for lang in langs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccf9835",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_BULK_SIZE = 1000\n",
    "MAX_BULK_UPLOADS = 1000\n",
    "\n",
    "sentinel = object()\n",
    "for j in range():\n",
    "    start = timer()\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        documents = []\n",
    "        while len(documents) < MAX_BULK_SIZE - len(all_langs_datasets):\n",
    "            for ds in all_langs_datasets:\n",
    "                text = next(ds, sentinel)\n",
    "                if text is not sentinel:\n",
    "                    documents.append(\n",
    "                        {\n",
    "                            \"_index\": \"hf-endpoint-index\",\n",
    "                            \"_source\": {\"text\": text['text']},\n",
    "                        }\n",
    "                    )\n",
    "                    # if you are using semantic text, use this append instead:\n",
    "                    # documents.append(\n",
    "                    #     {\n",
    "                    #         \"_index\": \"hf-semantic-text-index\",\n",
    "                    #         \"_source\": {\"infer_field\": text['text']},\n",
    "                    #     }\n",
    "                    # )\n",
    "\n",
    "        try:\n",
    "            response = helpers.bulk(client, documents, raise_on_error=False, timeout=\"60s\")\n",
    "\n",
    "            print(\"Doc set:\", j, end=\"; \")\n",
    "            end=timer()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"exception:\", str(e))\n",
    "            time.sleep(30)\n",
    "    except Exception as e:\n",
    "        print(j, e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0f6df7",
   "metadata": {},
   "source": [
    "## Semantic search\n",
    "\n",
    "After the dataset has been enriched with the embeddings, you can query the data using [semantic search](https://www.elastic.co/guide/en/elasticsearch/reference/current/knn-search.html#knn-semantic-search). Pass a `query_vector_builder` to the k-nearest neighbor (kNN) vector search API, and provide the query text and the model you have used to create the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b21b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.search(\n",
    "    index=\"hf-endpoint-index\",\n",
    "    size=3,\n",
    "    knn={\n",
    "        \"field\": \"text_embedding\",\n",
    "        \"query_vector_builder\": {\n",
    "            \"text_embedding\": {\n",
    "                \"model_id\": \"my_hf_endpoint_object\",\n",
    "                \"model_text\": \"Fighting movie\",\n",
    "            }\n",
    "        },\n",
    "        \"k\": 10,\n",
    "        \"num_candidates\": 100,\n",
    "    },\n",
    ")\n",
    "\n",
    "for hit in response[\"hits\"][\"hits\"]:\n",
    "    doc_id = hit[\"_id\"]\n",
    "    score = hit[\"_score\"]\n",
    "    text = hit[\"_source\"][\"text\"]\n",
    "    print(f\"Score: {score}\\nText: {text}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4055ba",
   "metadata": {},
   "source": [
    "**NOTE:** The value of `model_id` in the `query_vector_builder` must match the value of `inference_id` you created in the [first step](#create-the-inference-endpoint)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
